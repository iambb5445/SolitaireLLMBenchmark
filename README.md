# Solitaire LLM Benchmark

The code and dataset available here are part of the research presented in "LLM Game Rule Understanding through Out-of-Distribution Fine-Tuning" by Bahar Bateni, Benjamin Pratt and Jim Whitehead, published at the 21st AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment (AIIDE), 2025. The paper explortes the evaluation and improvement of rule-understanding abilities of large language models (LLMs) using multiple variants of Solitaire card games as testbeds. Despite the simplicity of rules, these games have numerous variations, each played very differently, making them ideal for our research questions. For more detail about our motivation and results, please refer to the paper available here (link to be added later).

## Overview
Our work on this research includes multiple components, spread across different sources.
1. The [Solitaire framework](https://github.com/iambb5445/SolitaireGDL) allows for creating and simulating many different variants using a custom Solitaire Game Description Langauge (SGDL) for defining the rules. It can also create datasets based on these simulations, which we include and use here.
2. <ins>**The current repository**</ins> contains code for using the datasets created with the Solitaire framework for testing and training Large Language Models.
3. Datasets used in the paper are included both on [huggingface](https://huggingface.co/datasets/bbateni/Solitaire-Rule-Reasoning-Benchmark) and [in the current repository](dataset).
4. Finally, a web version of our Solitaire framework can be accessed via this [link](https://iambb5445.github.io/SolitaireGDLWeb/index.html). Note that this version only allows for playing games and modifying game rules by changing the underlying SGDL, but does not provde a way to simulate the game using bots or create datasets.

<img width="1312" height="331" alt="image" src="https://github.com/user-attachments/assets/37853580-3695-4b96-a561-9e103c8f7d44" />

The code in this repository is for trainig and testing LLMs using the datasets generated by the other repository.

## Datasets

### Variants

As mentioned before, one of the reasons we choose Solitaire as our testbed is because it has many variants that are played very differently. We choose 6 variants for testing, and we use different sets from 13 variants for training. Part of out experiments are focused on how much we can push the way games in the training and test set are played further and still benefit from fine-tuning, hence the use of different training sets. Please refer to the paper for more information on our motivations and results. Each file in the training and test datasets represents one of these variants, and contains 1000 sampled states for it.

### Schema

For each file,
* "name": indicates the name of the variant
* "bot": shows the name of the bot used to play the game 100 times and generate the samples
* "description": a simple text-based description of the game, including definition of terms, list of possible moves and move conditions, and win conditions
* "samples": 1000 samples, each showing a possible state that was reached during one of the 100 simulated games. Each sample includes:
  * "game_id": a number in [0, 100) showing which one of the 100 simulated games this sample belongs to
  * "current_state_view": the current state of the game, including all information
  * "current_game_view": the current state of the game, but with partial information: face-down cards are indicated with "?"
  * "action": the proposed action to be performed
  * "summary": a summary of what this action means, which conditions it needs to follow, and whether or not it follows those conditions. This text fully explains whether or not the action is valid, and why.
  * "is_valid": whether or not the proposed action is valid according to the game rules and action conditions. The actions are chosen in a way to have around a 50-50 split between valid and invalid actions.
  * "next_state_view": the next state of the game if the action is performed, including all information (this is null if the action is invalid)
  * "next_game_view": the next state of the game, but with partial information: face-down cards are indicated with "?" (this is null if the action is invalid)

### Game Progression Questions

The datasets represent a set of game progression scenarios. For each sample, the input is the state and the proposed action for that state. The game progression question asks if the action is valid in this state, and if so, what is the resulting state from performing the action. The goal is for the LLM to consider the rules of the game to answer these questions. Our findings have shown that LLMs perform poorly in this task.

## How to Benchmark

### Tokens
Before benchmarking, you need to set your tokens for connection to any LLM want to test. This is done by changing [`auth.py`](auth.py) tokens for any platform used.

### Run the Experiment
Using one of the existing datasets (or creating your own using the [Solitaire framework](https://github.com/iambb5445/SolitaireGDL)) you can benchmark an llm on answer these questions using:

```
usage: run_experiment.py [-h] [--prompt-filename PROMPT_FILENAME] [--fewshot-seed FEWSHOT_SEED]
                         [--fewshot-size FEWSHOT_SIZE] [--max-fail-count MAX_FAIL_COUNT] [--thread-count THREAD_COUNT]     
                         [--unified-fewshot] [--non-uniform-game-id]
                         dataset-filename max-count seed llm

positional arguments:
  dataset-filename      The dataset json file
  max-count             Number of samples to use from the dataset
  seed                  Seed for choosing the samples
  llm                   Model to use. Options are: dict_keys([<OpenAIModel.GPT_4O_mini: 'gpt-4o-mini'>,
                        <OpenAIModel.GPT_4O: 'gpt-4o'>, <OpenAIModel.GPT_Turbo_35: 'gpt-3.5-turbo'>,
                        <DeepSeekModel.DEEP_SEEK_CHAT: 'deepseek-chat'>, <DeepSeekModel.DEEP_SEEK_REASONER: 'deepseek-     
                        reasoner'>, <GeminiModel.Gemini_15_Flash_002: 'google/gemini-1.5-flash-002'>,
                        <GeminiModel.Gemini_15_Pro_002: 'google/gemini-1.5-pro-002'>, <DeepInfraModel.LLAMA_33_70B_I:      
                        'meta-llama/Llama-3.3-70B-Instruct'>, <DeepInfraModel.LLAMA_3_70B_I: 'meta-llama/Meta-
                        Llama-3-70B-Instruct'>, <DeepInfraModel.LLAMA_3_8B_I: 'meta-llama/Meta-Llama-3-8B-Instruct'>,      
                        <DeepInfraModel.GEMINI_25_FLASH: 'google/gemini-2.5-flash'>]). Use code to add more.

options:
  -h, --help            show this help message and exit
  --prompt-filename PROMPT_FILENAME
                        Text file containing the prompt. You may use a new prompt. Special keywords {game_name} and        
                        {game_desc} will be replaced with the name of the game and the text description of the rules       
                        respectively.
  --fewshot-seed FEWSHOT_SEED
                        Seed used to choose the fewshot samples
  --fewshot-size FEWSHOT_SIZE
                        Number of fewshot samples to use for each request
  --max-fail-count MAX_FAIL_COUNT
                        How many failed attempts (connection issues, invalid response format, etc.) should be tolerated    
                        before aborting a request
  --thread-count THREAD_COUNT
                        Number of threads to parallelize the requests
  --unified-fewshot     Whether or not to use the same fewshot examples for all requests
  --non-uniform-game-id
                        If all samples aren't used, whether or not to try using the same number of samples from each game  
```

The default promp can be viewed in [`prompt.txt`](prompt.txt). After benchmarking an llm, an output dataset will be created. For each sample, this output dataset includes:
* All the fields from the input that were shared with the LLM
  * "state": current state of the game, previously "current_state_view"
  * "action": proposed action
* Any ground truth in the dataset, not sent to the LLM
  * "thinking": previously "summary"
  * "legal": previously "is_valid"
  * "next_state": previously "next_state_view"
* Responses from the LLM
  * "pred_state": LLMs idea of current state (repeating the given "state")
  * "pred_action": LLMs idea of current state (repeating the given "action")
  * "pred_thinking": LLMs explanation of the conditions of this action, and whether or not they are valid (enabling chain-of-thought reasoning)
  * "pred_legal": LLMs prediction on whether or not the action is legal
  * "pred_next_state": LLMs prediction of the next state

### Metrics

After the output is saved, it can be passed to another module to find out how the LLM has performed.

```
usage: report_experiment.py [-h] result_filename

positional arguments:
  result_filename  The result file

options:
  -h, --help       show this help message and exit
```

This will report on important metrics, the most important ones are:
* "legal accuracy": accuracy in predicting the legality of a move
* "legal next state accuracy": accuracy in predicting next state when the move is legal

Alternatively, you may use `report_mass_experiment.py` if your results are spread across multiple files:

```
usage: report_mass_experiment.py [-h] --results_dir RESULTS_DIR

options:
  -h, --help            show this help message and exit
  --results_dir RESULTS_DIR
                        Directory with experiment result JSON files
```

## How to Train

One of the standard formats for a fine-tuning file is organizing the requests and expected responsed in a jsonl format, for example [used by openai's platform](https://platform.openai.com/docs/guides/supervised-fine-tuning#formatting-your-data). To create such a file from a dataset, you can use:

```
usage: generate_finetune_samples.py [-h] [--prompt-filename PROMPT_FILENAME] [--fewshot-seed FEWSHOT_SEED]
                                    [--thread-count THREAD_COUNT] [--unified-fewshot] [--non-uniform-game-id]
                                    samples_per_file [dataset_filenames ...]

positional arguments:
  samples_per_file      Number of samples to use from the each dataset file
  dataset_filenames     All the dataset files

options:
  -h, --help            show this help message and exit
  --prompt-filename PROMPT_FILENAME
                        Text file containing the prompt. You may use a new prompt. Special keywords {game_name} and {game_desc}  
                        will be replaced with the name of the game and the text description of the rules respectively.
  --fewshot-seed FEWSHOT_SEED
                        Seed used to choose the fewshot samples
  --thread-count THREAD_COUNT
                        Number of threads to parallelize the requests
  --unified-fewshot     Whether or not to use the same fewshot examples for all requests
  --non-uniform-game-id
                        If all samples aren't used, whether or not to try using the same number of samples from each game
```

Note that you may want to use similar arguments to what you want to eventually use for benchmarking. For example, if you're testing the model on 4 few shot examples in training, you might also want to use 4 few shot examples when testing. Same for the prompt.

This step will not connect to any LLM, and does not require setting your authentication tokens to work.
